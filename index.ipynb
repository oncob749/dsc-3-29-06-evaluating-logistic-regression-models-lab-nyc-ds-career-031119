{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Logistic Regression Models - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As we saw with KNN, we need alternative evaluation metrics to determine the effectiveness of classification algorithms. In regression, we were predicting values so it made sense to discuss error as a distance of how far off our estimates were. In classifying a binary variable however, we are either correct or incorrect. As a result, we tend to deconstruct this as how many false positives versus false negatives we come across.  \n",
    "In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this review lab, we'll review precision, recall and accuracy in order to evaluate our logistic regression models.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "You will be able to:  \n",
    "* Understand and assess precision recall and accuracy of classifiers\n",
    "* Evaluate classification models using various metrics\n",
    "\n",
    "## Terminology Review  \n",
    "\n",
    "Let's take a moment and review some classification evaluation metrics:  \n",
    "\n",
    "\n",
    "$Precision = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}}$    \n",
    "  \n",
    "\n",
    "$Recall = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}}$  \n",
    "  \n",
    "$Accuracy = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}}$\n",
    "\n",
    "![](./images/Precisionrecall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At times, we may wish to tune a classification algorithm to optimize against precison or recall rather then overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is much preferable to optimize for recall, the number of cancer positive cases, then it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a standard logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    205\n",
      "1     37\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "import numpy as np\n",
    "#We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros.\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "# print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a function to calculate the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_hat, y):\n",
    "    tp = 0\n",
    "    pp = 0\n",
    "    for i in len(y_hat):\n",
    "        tp += 1 [if y_hat[i] == ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write a function to calculate the recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_hat, y):\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate the precision, recall and accuracy of your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision:  4      1.000000\n",
      "203         NaN\n",
      "287    0.000000\n",
      "113    1.000000\n",
      "16     1.000000\n",
      "148    1.000000\n",
      "215         NaN\n",
      "47     1.000000\n",
      "86     1.000000\n",
      "147    1.000000\n",
      "118    1.000000\n",
      "54     1.000000\n",
      "21     1.000000\n",
      "107    1.000000\n",
      "165         NaN\n",
      "130    1.000000\n",
      "232         NaN\n",
      "88     1.000000\n",
      "301         NaN\n",
      "155    1.000000\n",
      "67     1.000000\n",
      "12     1.000000\n",
      "108    1.000000\n",
      "225         NaN\n",
      "141    1.000000\n",
      "291         NaN\n",
      "302    0.000000\n",
      "33          inf\n",
      "60     1.000000\n",
      "23     1.000000\n",
      "         ...   \n",
      "162    1.000000\n",
      "252         NaN\n",
      "34          inf\n",
      "94     1.000000\n",
      "138         inf\n",
      "157    1.000000\n",
      "201         NaN\n",
      "289         NaN\n",
      "122    1.000000\n",
      "150         inf\n",
      "146    1.000000\n",
      "132    1.000000\n",
      "44     1.000000\n",
      "112    1.000000\n",
      "176         NaN\n",
      "214         NaN\n",
      "257         NaN\n",
      "15     1.000000\n",
      "247         NaN\n",
      "142    1.000000\n",
      "253         NaN\n",
      "238         NaN\n",
      "288         NaN\n",
      "81     1.000000\n",
      "259         NaN\n",
      "200    0.000000\n",
      "93     1.000000\n",
      "87     1.000000\n",
      "167         NaN\n",
      "25     1.000000\n",
      "Name: target, Length: 242, dtype: float64\n",
      "Testing Precision:  297         NaN\n",
      "26     1.000000\n",
      "198         NaN\n",
      "154    1.000000\n",
      "152    1.000000\n",
      "151    1.000000\n",
      "121    1.000000\n",
      "128    1.000000\n",
      "11     1.000000\n",
      "218         NaN\n",
      "276         NaN\n",
      "213         NaN\n",
      "212         NaN\n",
      "110    1.000000\n",
      "190         NaN\n",
      "268         NaN\n",
      "242         NaN\n",
      "43     1.000000\n",
      "204         NaN\n",
      "199    0.000000\n",
      "111    1.000000\n",
      "164         inf\n",
      "92          inf\n",
      "278    0.000000\n",
      "235         NaN\n",
      "106    1.000000\n",
      "124    1.000000\n",
      "104    1.000000\n",
      "251         NaN\n",
      "230    0.000000\n",
      "         ...   \n",
      "84     1.000000\n",
      "178         NaN\n",
      "27     1.000000\n",
      "293    0.000000\n",
      "161    1.000000\n",
      "219         NaN\n",
      "231         NaN\n",
      "240         NaN\n",
      "29     1.000000\n",
      "173         NaN\n",
      "163         inf\n",
      "42          inf\n",
      "125    1.000000\n",
      "79     1.000000\n",
      "181         NaN\n",
      "73     1.000000\n",
      "224         NaN\n",
      "28     1.000000\n",
      "66     1.000000\n",
      "221         NaN\n",
      "209         NaN\n",
      "256         NaN\n",
      "31     1.000000\n",
      "82     1.000000\n",
      "262         NaN\n",
      "18     1.000000\n",
      "234         NaN\n",
      "296    0.000000\n",
      "188    0.000000\n",
      "80     1.000000\n",
      "Name: target, Length: 61, dtype: float64\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "print('Training Precision: ', precision(y_hat_train, y_train))\n",
    "print('Testing Precision: ', precision(y_hat_test, y_test))\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Precision Recall and Accuracy of Test vs Train Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the precision, recall and accuracy for test and train splits using different train set sizes. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importimport  matplotlib.pyplotmatplot  as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_Precision = []\n",
    "testing_Precision = []\n",
    "training_Recall = []\n",
    "testing_Recall = []\n",
    "training_Accuracy = []\n",
    "testing_Accuracy = []\n",
    "\n",
    "for i in range(10,95):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) #replace the \"None\" here\n",
    "    logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "    model_log = None\n",
    "    y_hat_test = None\n",
    "    y_hat_train = None\n",
    "\n",
    "# 6 lines of code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 scatter plots looking at the test and train precision in the first one, test and train recall in the second one, and testing and training accuracy in the third one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for test and train precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for test and train recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for test and train accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Nice! In this lab, you gained some extra practice with evaluation metrics for classification algorithms. You also got some further python practice by manually coding these functions yourself, giving you a deeper understanding of how they work. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
